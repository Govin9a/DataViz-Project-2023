{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ce3a403-0006-4fb4-914f-2e503b770a7f",
   "metadata": {},
   "source": [
    "# Data Collection Master Notebook\n",
    "\n",
    "This dataset primarily revolves around the lab events and free notes found in MIMIC-III, located in the labeevents and noteevents tables in the database.\n",
    "\n",
    "This notebook shares our data collection methods used in each of our visualizations. Each of the group members curated a dataset to be used and each are outlined as below. However, running the notebook is not recommended as some processes take a long time to run. Just use the finished file shared in the 'Data/' folder located in this GitHub repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f723774b-5bb2-4c6b-822a-8754ff8e1115",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311db286-0c2e-4f8c-a936-f747c67d3e9c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb649a37-5c16-4f40-a4f1-62beede418a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021277ff-4b50-47bf-94f9-6a772f8217d9",
   "metadata": {},
   "source": [
    "### Sqlite3 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f23a11-5838-448c-922a-29eeee42ad61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relative path to where the mimic3.db file is\n",
    "db_path = '/mnt/f/mimic-iii-clinical-database-1.4/mimic3.db'\n",
    "#connection object to db\n",
    "sqliteConnection = sqlite3.connect(db_path)\n",
    "#cursor/pointer\n",
    "mimiciii = sqliteConnection.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1862184d-0b98-40be-9b8f-2a57a3759876",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e9df21-91e5-4b09-a60d-d8640bf56c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_col_names(cursor, table_name):\n",
    "    '''\n",
    "    Retrieves the column names for a table in a sqlite3 db.\n",
    "    ------\n",
    "    cursor: sqliteConnection cursor object\n",
    "    table_name: table_name to get column names for\n",
    "    '''\n",
    "    cursor.execute(f\"\"\"\n",
    "    SELECT sql FROM sqlite_master WHERE name='{table_name}';\n",
    "    \"\"\")\n",
    "    \n",
    "    res = mimiciii.fetchall()\n",
    "    cols = re.findall(r'\\\"\\w+\\\"', res[0][0])\n",
    "    return [x[1:-1] for x in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cdb4b5-5274-4883-8cab-6f763386a42a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def condense_notes(admission_df, noteevents_df):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    admission_ids = admission_df.HADM_ID.unique()\n",
    "    admission_ids.sort()\n",
    "\n",
    "    condensed_notes = pd.DataFrame()\n",
    "    condensed_notes['HADM_ID'] = admission_ids\n",
    "    notes_list = []\n",
    "    \n",
    "    for adm_id in tqdm(admission_ids):\n",
    "        curr_adm = noteevents[noteevents.HADM_ID == adm_id]\n",
    "        categories = curr_adm.CATEGORY.unique()\n",
    "        curr_chart = ''\n",
    "        \n",
    "        for category in categories:\n",
    "            curr_chart += '----CATEGORY: ' + category + '----\\n\\n'\n",
    "            curr_category_notes = curr_adm[curr_adm.CATEGORY == category][['DESCRIPTION', 'TEXT']]\n",
    "            curr_descriptions = curr_category_notes.DESCRIPTION.to_list()\n",
    "            curr_notes = curr_category_notes.TEXT.to_list()\n",
    "\n",
    "            for i in range(len(curr_descriptions)):\n",
    "                curr_chart += '--NEW: ' + curr_descriptions[i] + '--\\n'\n",
    "                curr_chart += curr_notes[i] + '\\n'\n",
    "\n",
    "            curr_chart += '\\n\\n'\n",
    "\n",
    "        notes_list.append(curr_chart)\n",
    "    condensed_notes['TEXT'] = notes_list\n",
    "    \n",
    "    return condensed_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f6f34c-46a8-4dae-a3a1-d2acd009aca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_from_table_from_db(cursor, table_name, num_rows='*', skip_cols=[]):\n",
    "    '''\n",
    "    Retreives table from sqlite3 db in form of df\n",
    "    ------\n",
    "    cursor: sqliteConnection cursor object\n",
    "    table_name: name of table to get from cursor db\n",
    "    num_rows: number of rows to retrieve (or '*' for all rows)\n",
    "    skip_cols: list of columns to skip in the retrieval\n",
    "    '''\n",
    "    col_names = get_col_names(cursor, table_name)\n",
    "    \n",
    "    use_cols = [col for col in col_names if col not in skip_cols]\n",
    "    \n",
    "    if num_rows == '*':\n",
    "        query = f'''select {', '.join(use_cols)} from {table_name};'''\n",
    "    else:\n",
    "        query = f'''select {', '.join(use_cols)} from {table_name} limit {num_rows};'''\n",
    "        \n",
    "    cursor.execute(query)\n",
    "    rows = cursor.fetchall()\n",
    "    \n",
    "    return pd.DataFrame(rows, columns=use_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ec5cb9-4f42-42d4-8211-1f6f1f23869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables_list_from_db(cursor):\n",
    "    cursor.execute(\"\"\"\n",
    "    select name from sqlite-master where type='table';\n",
    "    \"\"\")\n",
    "    table_names = [table[0] for table in cursor.fetchall()]\n",
    "    return table_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4697c677-a5c9-40b0-b69c-c66e3fdc60bc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Get HADM_IDs associated with ARF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491aa478-2bca-4335-9a16-99e2d13396e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all admissions\n",
    "admission = get_df_from_table_from_db(mimiciii, 'admissions')\n",
    "admission = admission.apply(lambda x: x.astype(str).str.upper())\n",
    "admission.SUBJECT_ID = admission.SUBJECT_ID.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bc3cf3-3fe7-454f-a824-e3623a02b38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for ARF-related diagnoses\n",
    "arf_adm= admission[admission.DIAGNOSIS.str.contains('RESPIRATORY FAILURE') | admission.DIAGNOSIS.str.contains('RESP. FAILURE')  | admission.DIAGNOSIS.str.contains('RESP FAILURE')]\n",
    "to_exclude = ['CHRONIC RESPIRATORY FAILURE;AIRWAY OBSTRUCTION', 'CHRONIC RESPIRATORY FAILURE', 'CHRONIC RESPIRATORY FAILURE; TRAC OBSTRUCTED AIRWAY']\n",
    "arf_adm = arf_adm[~arf_adm.DIAGNOSIS.isin(to_exclude)]\n",
    "arf_adm = arf_adm[arf_adm.HADM_ID != 0]\n",
    "\n",
    "#get HADM_IDs for admissions associated with ARF\n",
    "arf_adm_ID = arf_adm.HADM_ID.to_list()\n",
    "\n",
    "arf_adm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd5a679-dabc-4dfe-b82d-409609a1f9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all drg codes\n",
    "drgcodes = get_df_from_table_from_db(mimiciii, 'drgcodes')\n",
    "drgcodes.DESCRIPTION = drgcodes.DESCRIPTION.astype(str)\n",
    "\n",
    "#filter for ARF\n",
    "arf_drg = drgcodes[drgcodes.DESCRIPTION.str.contains('RESPIRATORY FAILURE')]\n",
    "arf_drg = arf_drg[arf_drg.HADM_ID != 0]\n",
    "\n",
    "#get HADM_IDs for drg codes associated with ARF\n",
    "arf_drg_ID = arf_drg.HADM_ID.to_list()\n",
    "\n",
    "arf_drg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2806a807-e8e9-4c42-8013-aa53a751c63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine drg and admission HADM_IDs associated with ARF\n",
    "arf_hadm_ids = list(set(arf_drg_ID + arf_adm_ID))\n",
    "\n",
    "#get all other diagnosis IDs\n",
    "other_hadm_ids = list(set([hadm_id for hadm_id in admission.HADM_ID if hadm_id not in arf_hadm_ids]))\n",
    "\n",
    "#save HADM_IDs associated with ARF to 'arf_hadm_ids.json' and OTHER diagnoses to 'other_hadm_ids.json'\n",
    "with open('Data/arf_hadm_ids.json', 'w') as j_file:\n",
    "    json.dump(arf_hadm_ids, j_file, indent=4)\n",
    "\n",
    "with open('Data/other_hadm_ids.json', 'w') as j_file:\n",
    "    json.dump(other_hadm_ids, j_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7b399a-e2e8-4df7-bd2f-766d1be52f0d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Govinda\n",
    "\n",
    "To process 'Data/DataViz_Project_Data-Frame_Govinda.csv' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e493de-812b-4b39-b1ff-0a4c8e16aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = 'Data/arf_hadm_ids.json'\n",
    "\n",
    "with open(json_path) as f:\n",
    "    arf_hadm_ids = json.load(f)\n",
    "\n",
    "# Convert 'hadm_id' values to integers\n",
    "arf_hadm_ids = list(map(int, arf_hadm_ids))\n",
    "\n",
    "# Load labevents CSV data\n",
    "#labevents_df = pd.read_csv('LABEVENTS.csv')\n",
    "labevents_df = get_df_from_table_from_db(mimiciii, 'labevents')\n",
    "\n",
    "# Handle non-finite values in 'hadm_id' column\n",
    "labevents_df['hadm_id'] = pd.to_numeric(labevents_df['hadm_id'], errors='coerce')\n",
    "labevents_df = labevents_df.dropna(subset=['hadm_id'])\n",
    "\n",
    "# Convert 'hadm_id' column to integers\n",
    "labevents_df['hadm_id'] = labevents_df['hadm_id'].astype(int)\n",
    "\n",
    "# Filter labevents_df based on arf_hadm_ids\n",
    "arf_dataframe = labevents_df[labevents_df['hadm_id'].isin(arf_hadm_ids)]\n",
    "\n",
    "# Save the filtered data to a CSV file\n",
    "arf_csv_path = 'Data/arf_dataframe.csv'\n",
    "#arf_dataframe.to_csv(arf_csv_path, index=False)\n",
    "\n",
    "# Read the arf dataset \n",
    "#arf_dataframe = pd.read_csv(arf_csv_path)\n",
    "\n",
    "# Display the number of rows in arf_dataframe\n",
    "print(\"Number of rows in arf_dataframe:\", len(arf_dataframe))\n",
    "\n",
    "print(arf_dataframe.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86386a37-1e76-452e-b668-6a9c2bbb2e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data for patients with diagnoses other than ARF\n",
    "other_diagnoses = labevents_df[~labevents_df['hadm_id'].isin(arf_hadm_ids)]\n",
    "other_diagnoses\n",
    "\n",
    "# Save the data for other diagnoses to a CSV file\n",
    "other_diagnoses_csv_path = 'Data/other_diagnoses.csv'\n",
    "#other_diagnoses.to_csv(other_diagnoses_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42964e24-da2d-41b8-b9fb-0b4182f67119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop blank values in 'valuenum' column\n",
    "arf_dataframe['valuenum'].dropna(inplace=True)\n",
    "\n",
    "# Group by 'itemid' and calculate the average of 'valuenum'\n",
    "arf_average_value = arf_dataframe.groupby('itemid')['valuenum'].mean().reset_index()\n",
    "\n",
    "# Add a new column 'diagnosis' with the label 'arf' for each row\n",
    "arf_average_value['diagnosis'] = ['arf' for x in range(len(arf_average_value))]\n",
    "\n",
    "# Save the average values to a CSV file\n",
    "average_csv_path = 'Data/arf_average_value.csv'\n",
    "#arf_average_value.to_csv(average_csv_path, index=False)\n",
    "\n",
    "# Read the average values dataset\n",
    "#arf_average_value = pd.read_csv(average_csv_path)\n",
    "\n",
    "# Display the resulting DataFrame with average values\n",
    "arf_average_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8b268d-d59c-4c8a-b59d-3117aa4bd7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop blank values in 'valuenum' column for other diagnoses\n",
    "other_diagnoses['valuenum'].dropna(inplace=True)\n",
    "\n",
    "# Group by 'itemid' and calculate the average of 'valuenum'\n",
    "other_diagnoses_average_value = other_diagnoses.groupby('itemid')['valuenum'].mean().reset_index()\n",
    "\n",
    "# Add a new column 'diagnosis' with the label 'other' for each row\n",
    "other_diagnoses_average_value['diagnosis'] = ['other' for x in range(len(other_diagnoses_average_value))]\n",
    "\n",
    "# Save the average values to a CSV file for other diagnoses\n",
    "other_average_csv_path = 'Data/other_diagnoses_average_value.csv'\n",
    "#other_diagnoses_average_value.to_csv(other_average_csv_path, index=False)\n",
    "\n",
    "# Read the average values dataset for other diagnoses\n",
    "#other_diagnoses_average_value = pd.read_csv(other_average_csv_path)\n",
    "\n",
    "# Display the resulting DataFrame with average values for other diagnoses\n",
    "other_diagnoses_average_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8340269-d991-468e-8a5c-542e6d60a5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the ARF average values and other diagnoses average values\n",
    "combined_df = pd.concat([arf_average_value, other_diagnoses_average_value], ignore_index=True)\n",
    "combined_df = pd.pivot_table(combined_df, index='itemid', columns='diagnosis', values='valuenum', aggfunc='first')\n",
    "combined_df.reset_index(inplace=True)\n",
    "combined_df.columns.name = None\n",
    "combined_df.columns = ['itemid', 'arf', 'other']\n",
    "#Save the combined DataFrame to a CSV file\n",
    "combined_csv_path = 'Data/combined_df.csv'\n",
    "#combined_df.to_csv(combined_csv_path, index=False)\n",
    "\n",
    "# Read the combined dataset\n",
    "#combined_df = pd.read_csv(combined_csv_path)\n",
    "\n",
    "# Display the resulting combined DataFrame\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2e0b06-d726-499e-9f0c-6e07ad853288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading Lab Items Data\n",
    "#labitems_df = pd.read_csv('D_LABITEMS.csv')\n",
    "labitems_df = get_df_from_table_from_db(mimiciii, table_name='d_labitems')\n",
    "labitems_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a53f29f-d312-405d-955b-ae988ba0bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with Lab Item data\n",
    "DataFrame = combined_df.merge(labitems_df[['itemid', 'label']], on='itemid', how='left')\n",
    "\n",
    "# Display the resulting DataFrame with selected column\n",
    "DataFrame[['itemid', 'label', 'arf', 'other']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddac3ec-d907-4c81-89e9-d6ad17ff8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "DataFrame_csv_path = 'Data/DataViz_Project_DataFrame_Govinda.csv'\n",
    "DataFrame.to_csv(DataFrame_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e3f6b6-e3ad-4316-8c1c-7f3a82914c0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bac4b3-759b-4c08-bf42-395b84c0b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract admission table with designated attributes\n",
    "admission = get_df_from_table_from_db(mimiciii, 'admissions')\n",
    "admission = admission.apply(lambda x: x.astype(str).str.upper())\n",
    "admission.HADM_ID = admission.HADM_ID.astype('int64')\n",
    "admission.SUBJECT_ID = admission.SUBJECT_ID.astype('int64')\n",
    "\n",
    "admission = admission[['HADM_ID','SUBJECT_ID','DIAGNOSIS']]\n",
    "admission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e242b4-08b2-4162-a207-43e522f56b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import json for HADM_ID list of arf diagnosis\n",
    "arf_data = []\n",
    "with open('arf_hadm_ids.json') as json_file:\n",
    "   arf_data = json.load(json_file)\n",
    "admission.loc[admission['HADM_ID'].isin(list(map(int, arf_data)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47184b2e-392d-4755-98e1-d72c8a015121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mark arf as arf, and the rest as other\n",
    "admission['WR_DIAGNOSIS'] = np.full(len(admission),\"Other\")\n",
    "admission.loc[admission['HADM_ID'].isin(list(map(int, arf_data))),'WR_DIAGNOSIS'] = \"ARF\" \n",
    "admission['WR_DIAGNOSIS'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566385f-caaf-4af0-af6c-0c3583122a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the labevents table, which is huge\n",
    "# labevents = get_df_from_table_from_db(mimiciii, 'labevents')\n",
    "# labevents = labevents[~np.isnan(labevents['HADM_ID'])]\n",
    "# labevents.HADM_ID = labevents.HADM_ID.astype('int64')\n",
    "# labevents.to_csv(\"temp_labevent.csv\",index=False)\n",
    "# labevents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fa36e6-222c-4d14-b418-5b42389901de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the labitems table for ITEMID-test name mapping\n",
    "test_names = get_df_from_table_from_db(mimiciii, 'd_labitems')\n",
    "test_names.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc602e1-c764-4944-8d15-be495eefccdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are too many tests types and not all of them are performed for a HADM_ID\n",
    "# So instead, we pick the most common 10 tests to decrease the number of dropped tests when doing dropna \n",
    "most_common_tests = labevents.groupby('ITEMID').count().sort_values(by='HADM_ID', ascending=False)[0:10]\n",
    "most_common_tests = pd.DataFrame({'ITEMID':most_common_tests.index,'COUNT':most_common_tests['HADM_ID']})\n",
    "most_common_tests.reset_index(drop=True,inplace=True)\n",
    "most_common_tests = most_common_tests.merge(test_names,on='ITEMID',how='left')[['ITEMID','LABEL']]\n",
    "most_common_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174019fb-326d-476b-9d5c-e5f12a716d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the labevents according to the most common tests\n",
    "labevents = labevents[labevents['ITEMID'].isin(most_common_tests['ITEMID'].unique())]\n",
    "labevents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e0090-df69-4af7-b7ee-d05433d2c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Align two tables\n",
    "labevents = labevents[labevents['HADM_ID'].isin(admission['HADM_ID'].unique())]\n",
    "admission = admission[admission['HADM_ID'].isin(labevents['HADM_ID'].unique())]\n",
    "print(len(labevents['HADM_ID'].unique()))\n",
    "print(len(admission['HADM_ID'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b64352-daff-4168-84b8-1b4ec4b523b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make columns for most common tests' values and abnormalities\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "for name in most_common_tests['LABEL']:\n",
    "    admission.loc[:,name] = np.full(len(admission),\"\")\n",
    "    admission.loc[:,name+\"_ab\"] = np.full(len(admission),False)\n",
    "admission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24ba04d-ddaa-446b-8744-73e21bfe239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect values form labevents and fill them into admission dataframe\n",
    "for id in admission['HADM_ID']:\n",
    "    temp = labevents[labevents['HADM_ID'] == id]\n",
    "    for item in most_common_tests['ITEMID']:\n",
    "        row = admission['HADM_ID'] == id\n",
    "        col = most_common_tests[most_common_tests['ITEMID'] == item]['LABEL'].values[0]\n",
    "        if len(temp[temp['ITEMID'] == item]['VALUE'].values) <= 0:\n",
    "            admission.loc[row, col] = np.NaN\n",
    "            admission.loc[row, col+'_ab'] = np.NaN\n",
    "        else:\n",
    "            admission.loc[row, col] = temp[temp['ITEMID'] == item]['VALUE'].values[0]\n",
    "            admission.loc[row, col+'_ab'] = temp[temp['ITEMID'] == item]['FLAG'].values[0] == \"abnormal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d411e-a06e-4550-97f3-317be76604d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data for later use\n",
    "admission.to_csv(\"Data/ED_Wrangling_Result.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583cfb3a-bdd4-4dba-9b5d-ae96d6f5ce65",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Kolton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce3206d-1f48-4c2b-845f-de74e051f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all noteevents\n",
    "noteevents = get_df_from_table_from_db(mimiciii, 'noteevents')\n",
    "noteevents.HADM_ID = noteevents.HADM_ID.apply(fix_dot_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90464482-f871-4145-bda7-1629a6a765df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#condense/combine all notes associated with single HADM_ID into one note\n",
    "condensed_notes = condense_notes(admission, noteevents)\n",
    "for index, row in condensed_notes.iterrows():\n",
    "    with open(f'all_notes/{row[\"HADM_ID\"]}.txt', 'w') as f:\n",
    "        f.write(row['TEXT'])\n",
    "\n",
    "#save that file\n",
    "condensed_notes.to_csv('Data/all_notes_raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81bfc14-f1b6-4c00-b006-fb88bfcbb2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = condensed_notes.head(500)\n",
    "subset.to_csv('Data/subset_notes_raw.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
